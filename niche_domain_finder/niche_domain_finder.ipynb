{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Niche Domain Finder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1xlVkjLRe3bV0dpep+ArB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/searchsolved/search-solved-public-seo/blob/main/niche_domain_finder/niche_domain_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcLdrYselFLC"
      },
      "source": [
        "### Script by @LeeFootSEO\n",
        "**Early Access To Apps & Feature Requests:** https://www.patreon.com/leefootseo\n",
        "\n",
        "**Buy me a Coffee:** https://www.buymeacoffee.com/leefootseo\n",
        "\n",
        "# Niche Finder Pro\n",
        "by https://twitter.com/LeeFootSEO 10/06/2021 <- follow for more SEO Python Scripts.\n",
        "\n",
        "\n",
        "1.   Check if Domain is Available to Register (Free)\n",
        "2.   Appraise Domain Value (GoDaddy feature based on similar sites) (Free)\n",
        "3.   Check Keyword Slope (Free)\n",
        "4.   Scrape SERPs for Competiton (# of sites with exact KW in the title) (Paid)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40bjMu2JkqEP"
      },
      "source": [
        "!pip install pytrends\n",
        "!pip install pandas\n",
        "!pip install fabric3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Zl7trykjUV"
      },
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "from google.colab import files \n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "from datetime import datetime, time\n",
        "import time as t\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "from time import sleep\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pytrends.request import TrendReq\n",
        "from fabric.colors import green, red, magenta  # pip install fabric3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44JMtxjtPH1H"
      },
      "source": [
        "# Enter TLDs to check and bad words to block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAcw7PgIyHNx"
      },
      "source": [
        "bad_words = [\"free\", \"download\", \"pdf\"]  # enter bad words to filter\n",
        "extensions = [\"com\", \"co.uk\"]  #enter domain combinations to check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVABrYNDmJPr"
      },
      "source": [
        "#Enter Your API Details Below\n",
        "\n",
        "1.   GoDaddy API (Free) - https://developer.godaddy.com/getstarted\n",
        "2.   ZENSerp API (Paid) - https://zenserp.com/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAPVJ7tampO4"
      },
      "source": [
        "# godaddy developer key and secret (Mandatory)\n",
        "api_key = \"yourgodaddyapihere\"   # Your Key Here\n",
        "secret_key = \"yourgodaddysecrethere\"  # Your Key Here\n",
        "\n",
        "# zenserp key (paid) (Optional)\n",
        "zenserp_key = \"yourzenserpkeyhere\"  # Your Key Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tgUlufDLDmY"
      },
      "source": [
        "# Set GoDaddy API Delays (2-3 seconds is the sweet spot for GoDaddy!)\n",
        "domain_check_speed = 2  # godaddy API\n",
        "appraisal_check_speed = 4  # godaddy API\n",
        "\n",
        "# set godaddy vars\n",
        "chunk_size = 500  # number of domains to check in each call.\n",
        "max_length = 30  # filter domain names by length\n",
        "min_price = 0  # filter domain names by price range\n",
        "max_price = 5000  # filter domain names by price range\n",
        "min_appr_price = 0  # if appraisal is enabled, only include domain names with min appraisal price\n",
        "min_sale_price = 0  # when a domain is appraised, Godaddy API returns similar domains sold.\n",
        "min_sale_year = 2000  # when a domain is appraised, Godaddy API returns similar domains sold."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IRKq__KnQG3"
      },
      "source": [
        "# create empty lists / dictionaries to hold the data\n",
        "prefixes = []\n",
        "keywords = []\n",
        "suffixes = []\n",
        "all_domains = []\n",
        "similar_domains = []\n",
        "found_domains = {}\n",
        "domain_value = []\n",
        "# Domain availability and appraisal end points\n",
        "url = \"https://api.godaddy.com/v1/domains/available\"\n",
        "appraisal = \"https://api.godaddy.com/v1/appraisal/{}\"\n",
        "headers = {\"Authorization\": \"sso-key {}:{}\".format(api_key, secret_key)}  # godaddy api key is sent in the header\n",
        "pytrend = TrendReq()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id5nWp7NtS3U"
      },
      "source": [
        "# Import Keywords from Ahrefs Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soZVxWmMtSTV"
      },
      "source": [
        "uploaded = files.upload()\n",
        "df_keyword_list = list(uploaded.keys())[0]  # get the filename from the upload"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_er5RhOEtNz0"
      },
      "source": [
        "# read in the ahrefs export, massage the data to check domain availability.\n",
        "df_keywords = pd.read_csv(df_keyword_list)\n",
        "df_keywords['domain_keywords'] = df_keywords['Keyword']\n",
        "df_keywords['domain_keywords'] = df_keywords['domain_keywords'].apply(lambda x: x.replace(\" \", \"\"))\n",
        "keywords = list(df_keywords['domain_keywords'])\n",
        "kw_len = len(keywords)\n",
        "df_keywords = df_keywords.sort_values(by=\"Keyword\", ascending=True)\n",
        "df_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZS6bzfiyDne"
      },
      "source": [
        "# Generate domains\n",
        "for keyword in keywords:\n",
        "    for extension in extensions:\n",
        "        domain = \"{}.{}\".format(keyword, extension)\n",
        "        # Filter by length\n",
        "        if len(domain) <= max_length:\n",
        "            all_domains.append(domain)\n",
        "all_domains"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F68P2tEGytWL"
      },
      "source": [
        "# Remove blacklisted words\n",
        "df_all_domains = pd.DataFrame(all_domains, columns=['Keyword'])\n",
        "kws_before = (len(df_all_domains))\n",
        "bad_words = \"|\".join(bad_words)  # changes list into | separated string for matching\n",
        "df_all_domains = (df_all_domains[~df_all_domains.Keyword.str.contains(bad_words)]) #drops partial match words\n",
        "kws_after = (len(df_all_domains))\n",
        "all_domains = list(df_all_domains['Keyword'])  #changes df back into list\n",
        "kw_diff = kws_before - kws_after\n",
        "print(\"Removed\", kw_diff, \"Badwords!\")\n",
        "df_all_domains"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-vNpTZpzKGi"
      },
      "source": [
        "# This function splits all domains into chunks of a given size\n",
        "def chunks(array, size):\n",
        "    for i in range(0, len(array), size):\n",
        "        yield array[i:i + size]\n",
        "\n",
        "# Split the original array into subarrays\n",
        "domain_chunks = list(chunks(all_domains, chunk_size))\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(magenta(\"Checking which domains are available to register\"))\n",
        "print(\"--------------------------------------------------------\")\n",
        "# For each domain chunk (ex. 500 domains)\n",
        "for domains in domain_chunks:\n",
        "    # Get availability information by calling availability API\n",
        "    availability_res = requests.post(url, json=domains, headers=headers)\n",
        "    # Get only available domains with price range\n",
        "    for domain in json.loads(availability_res.text)[\"domains\"]:\n",
        "        if domain[\"available\"]:\n",
        "            price = float(domain[\"price\"]) / 1000000\n",
        "            if price >= min_price and price <= max_price:\n",
        "                print(\"{:30} : ${:10}\".format(domain[\"domain\"], price))\n",
        "                found_domains[domain[\"domain\"]] = price\n",
        "\n",
        "    # API call frequency should be ~ 30 calls per minute\n",
        "    t.sleep(domain_check_speed)\n",
        "\n",
        "# make dataframe from dictionary of available domains to register\n",
        "df_found_domains = pd.DataFrame.from_dict(found_domains, orient=\"index\")\n",
        "df_found_domains = df_found_domains.reset_index()\n",
        "df_found_domains\n",
        "\n",
        "df_found_domains.columns = ['Keyword', \"Price\"]\n",
        "  \n",
        "# output all available domains to register\n",
        "df_found_domains.to_csv('all_available_domains_to_register.csv')\n",
        "\n",
        "files.download('all_available_domains_to_register.csv')\n",
        "found_domains_len = len(found_domains)\n",
        "print(\"\")\n",
        "print(green((found_domains_len)) + green((\" of \")) + green((kw_len)) + green((\" domains are available to register!\")))\n",
        "print(\"\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(red(\"Exported all_available_domains_to_register.csv!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI99vKnW3pST"
      },
      "source": [
        "# Appraise the Value of the Domains - Free Via GoDaddy API\n",
        "Optional Domain Appraisal based on similar domains sold in the past."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROou88Bk3Flr"
      },
      "source": [
        "print(\"--------------------------------------------------------\")\n",
        "print(magenta(\"Appraising domain values based on similar sales\"))\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "for domain, price in found_domains.items():\n",
        "    # Call appraisal API\n",
        "    try:\n",
        "        appraisal_res = requests.get(appraisal.format(domain), headers=headers).json()\n",
        "    except Exception:\n",
        "        print(red(\"Error Getting Data!\"))\n",
        "        govalue = 100\n",
        "        pass\n",
        "    try:\n",
        "        # Get appraisal and similar sold domains\n",
        "        govalue = appraisal_res[\"govalue\"]\n",
        "        comparable_sales = appraisal_res[\"comparable_sales\"]\n",
        "\n",
        "    except:\n",
        "        # todo debug print\n",
        "        print(appraisal_res)\n",
        "        print(red(\"Error Getting Data! - line 192\"))\n",
        "        domain_value.append(\"Error Getting Data!\")\n",
        "        continue\n",
        "\n",
        "    # Filter by min appraisal price\n",
        "    if govalue >= min_appr_price:\n",
        "        domain_value.append(govalue)\n",
        "        print(\"{:30} : ${:10} : ${}\".format(domain, price, govalue))\n",
        "    for sale in comparable_sales:\n",
        "        # Filter similar sold domains by price and year\n",
        "        if sale[\"price\"] >= min_sale_price and sale[\"year\"] >= min_sale_year:\n",
        "            similar_domain = \"{:30} : {:10} : {:10}\".format(\n",
        "                sale[\"domain\"], sale[\"price\"], sale[\"year\"])\n",
        "            # Do not include duplicates\n",
        "            if similar_domain not in similar_domains:\n",
        "                similar_domains.append(similar_domain)\n",
        "\n",
        "    # Do not abuse the API!!!\n",
        "    t.sleep(appraisal_check_speed)\n",
        "\n",
        "df_found_domains[\"Appraised Value\"] = domain_value\n",
        "df_found_domains['temp_match'] = df_found_domains['Keyword'].str.split('.').str[0]\n",
        "\n",
        "df_final = pd.merge(df_found_domains, df_keywords, left_on=\"temp_match\", right_on=\"domain_keywords\", how=\"left\")\n",
        "\n",
        "# clean up and refine the data\n",
        "cols = \"Keyword_y\", \"Keyword_x\", \"Volume\", \"Global volume\", \"Price\", \"Appraised Value\", \"Country\", \"CPC\", \"Clicks\", \"Return Rate\"\n",
        "df_final = df_final.reindex(columns=cols)\n",
        "df_final.rename(columns={\"Keyword_y\": \"Keyword\", \"Keyword_x\": \"Domain Name\"}, inplace=True)\n",
        "df_final['Appraised Value'] = df_final['Appraised Value'].astype(str)\n",
        "df_final['Appraised Value'] = df_final['Appraised Value'].apply(lambda x: ','.join(map(str, x)))  # converts list values to str\n",
        "df_final['Appraised Value'] = df_final['Appraised Value'].apply(lambda x: x.replace(\",\", \"\"))\n",
        "df_final = df_final.sort_values(by=\"Appraised Value\", ascending=False)\n",
        "\n",
        "df_final.to_csv('all_available_domains_appraised.csv')\n",
        "files.download('all_available_domains_appraised.csv')\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(red(\"Exported appraised_domains_output.csv!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chZCDF-437HN"
      },
      "source": [
        "# Check Keyword Slope with PyTrends\n",
        "H/t to Paul Shapiro for the idea, and a lot of the code! https://searchwilderness.com/google-trends-api-slope/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGgfaRLTLpzy"
      },
      "source": [
        "# Set Google Trends Variable\n",
        "time_var = 'today 5-y'  # 'today #-m'  # 'all'  #\n",
        "geo_var = 'GB'\n",
        "random_delay_max = 3  # max random delay in seconds - pyTrends"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emA8aXWA3-la"
      },
      "source": [
        "# Start:: Keyword Slope ::\n",
        "df_read_csv = df_final[['Keyword']] \n",
        "searchKeywords = df_read_csv['Keyword'].tolist()\n",
        "# remove duplicates from list by creating a set then returning to a list\n",
        "searchKeywords = set(searchKeywords)\n",
        "searchKeywords = list(searchKeywords)\n",
        "\n",
        "def chunks(items, chunkSize):\n",
        "    for i in range(0, len(items), chunkSize):\n",
        "        yield items[i:i + chunkSize]\n",
        "\n",
        "list(chunks(searchKeywords, 5))\n",
        "\n",
        "# delete old chunk .csv files\n",
        "files = glob.glob('*chunk*.csv')\n",
        "for f in files:\n",
        "    os.remove(f)\n",
        "\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(magenta(\"Querying Google Trends via PyTrends\"))\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(\"Look Back Period:\", time_var)\n",
        "print(\"Geo Location:\", geo_var)\n",
        "print(\"Random Delay: 1 -\", random_delay_max, \"seconds\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# putting it all together (copying in calls to pytrends library to calculate interest_over_time)\n",
        "for chunkIndex, chunk in enumerate(chunks(searchKeywords, 5)):\n",
        "    # print('%2d) getting google trends for %s...' % (chunkIndex+1, chunk), end='')\n",
        "    print((chunk), end='')\n",
        "    chunkOutputFile = \"chunk%02d.csv\" % (chunkIndex + 1)\n",
        "    pytrend.build_payload(kw_list=chunk, timeframe=time_var, geo=geo_var)\n",
        "    interest_over_time_df = pytrend.interest_over_time()\n",
        "    interest_over_time_df.to_csv(path_or_buf=chunkOutputFile)\n",
        "    print(' - done! Saved to %s' % chunkOutputFile)\n",
        "    sleep(randint(1, random_delay_max))\n",
        "\n",
        "# concatenate chunked parts into single df\n",
        "all_files = glob.glob(\n",
        "    os.path.join(\"*chunk*.csv\")) \n",
        "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
        "\n",
        "concatenated_df = pd.concat(df_from_each_file, axis=1)\n",
        "concatenated_df_clean = (concatenated_df.drop('date', 1)).drop('isPartial', 1)\n",
        "\n",
        "df_dates_file = pd.read_csv('chunk01.csv')\n",
        "df_date_export = concatenated_df.iloc[:, 0]\n",
        "final_result = pd.concat([df_date_export, concatenated_df_clean], axis=1)\n",
        "\n",
        "# give me the current year and month\n",
        "yearToday = datetime.now().strftime('%Y')\n",
        "lastYear = (datetime.now() - relativedelta(years=1)).strftime('%Y')\n",
        "prevYear = (datetime.now() - relativedelta(years=2)).strftime('%Y')\n",
        "\n",
        "lastYearResults = final_result[(final_result['date'] > lastYear) & (final_result['date'] < yearToday)]\n",
        "prevYearResults = final_result[(final_result['date'] > prevYear) & (final_result['date'] < lastYear)]\n",
        "keywords_to_check = list(final_result)\n",
        "\n",
        "last_year_mean = lastYearResults[keywords_to_check].mean()\n",
        "prev_year_mean = prevYearResults[keywords_to_check].mean()\n",
        "\n",
        "xlast = last_year_mean\n",
        "xprev = prev_year_mean\n",
        "ylast = int(lastYear)\n",
        "yprev = int(prevYear)\n",
        "\n",
        "def slope_formula(xlast, xprev, ylast, yprev):\n",
        "    return (xlast - xprev) / (ylast - yprev)\n",
        "\n",
        "keywordFinallist = ((slope_formula(xlast, xprev, ylast, yprev)))\n",
        "\n",
        "df_keywordFinallist = pd.DataFrame(keywordFinallist)\n",
        "df_keywordFinallist = df_keywordFinallist.reset_index()\n",
        "df_keywordFinallist.columns = ['Keyword', 'Slope']\n",
        "\n",
        "# combine the godaddy and gtrends dataframes\n",
        "df_combined = pd.merge(df_final, df_keywordFinallist, on=\"Keyword\", how=\"left\")\n",
        "\n",
        "# delete old chunk .csv files\n",
        "files = glob.glob('*chunk*.csv')\n",
        "for f in files:\n",
        "    os.remove(f)\n",
        "df_combined.round(2)\n",
        "\n",
        "#export trend slope dataframe\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(red(\"Exported trends_slope.csv\"))\n",
        "df_combined.to_csv(\"trends_slope.csv\")\n",
        "files.download('trends_slope.csv')\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(red(\"Exported trends_slope.csv!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsFE_8MjEHjm"
      },
      "source": [
        "# Scrape Live SERPS Using ZenSERP API\n",
        "Used to check the competition. How many Websites have the keywords in the page title. Low number = Lower Competition in the SERPs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9vzFeNhJRlc"
      },
      "source": [
        "# Set ZEN SERP Variables\n",
        "device_var = \"Desktop\"\n",
        "search_engine_var = \"google.co.uk\"\n",
        "location_var = \"London,England,United Kingdom\"\n",
        "geo_location_Var = \"GB\"\n",
        "homepage_language_var = \"en\"\n",
        "\n",
        "# ZEN SERP API options\n",
        "limit_api_searches = True  # use to limit the number of Zen SERP api calls\n",
        "api_credits = 20  # if API Limited, how many credits?\n",
        "sort_on = \"Volume\"  # \"Volume\", \"Global volume\", \"Price\", \"Appraised Value\", \"Country\", \"CPC\", \"Clicks\", \"Return Rate\", \"Slope\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMvunt4HEEvU"
      },
      "source": [
        "# Start:: Zen SERP\n",
        "if not limit_api_searches:\n",
        "    api_credits = found_domains_len\n",
        "\n",
        "if api_credits > found_domains_len:\n",
        "    api_credits = found_domains_len\n",
        "\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(magenta((\"Scraping the SERPs! This will use \")) + magenta((api_credits)) + magenta((\" Credit(s)\")))\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(\"Device:\", device_var)\n",
        "print(\"Search Engine:\", search_engine_var)\n",
        "print(\"Location:\", location_var)\n",
        "print(\"Geo Location:\", geo_location_Var)\n",
        "print(\"Language:\", homepage_language_var)\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "df_combined = df_combined.sort_values(by=sort_on, ascending=False)\n",
        "df_combined.drop_duplicates(subset=['Keyword'], keep=\"first\", inplace=True)\n",
        "search_terms = df_combined['Keyword'].tolist()  # dump search term queries to a list\n",
        "\n",
        "# trim list based on specified credits\n",
        "search_terms = search_terms[:api_credits]\n",
        "\n",
        "# make empty list and dataframe to store the extracted data\n",
        "df_final = pd.DataFrame(None)\n",
        "url_list = []\n",
        "title_list = []\n",
        "query_list = []\n",
        "progress_count = 0\n",
        "\n",
        "for i in search_terms:\n",
        "    headers = {\"apikey\": zenserp_key}\n",
        "    params = (\n",
        "        (\"q\", i),\n",
        "        (\"device\", device_var),\n",
        "        (\"search_engine\", search_engine_var),\n",
        "        (\"location\", location_var),\n",
        "        (\"gl\", geo_location_Var),\n",
        "        (\"hl\", homepage_language_var),\n",
        "        (\"apikey\", zenserp_key),\n",
        "    )\n",
        "    progress_count = progress_count +1\n",
        "    print(i.strip(), \"- Searched\", progress_count, \"of\", found_domains_len)\n",
        "    response = requests.get('https://app.zenserp.com/api/v2/search', headers=headers, params=params);\n",
        "\n",
        "    # Get JSON Data\n",
        "    d = response.json()\n",
        "    json_str = json.dumps(d)  # dumps the json object into an element\n",
        "    resp = json.loads(json_str)  # load the json to a string\n",
        "    organic = (resp['organic'])\n",
        "\n",
        "    # get the length of the list to iterate over in the loop\n",
        "    list_len = len(organic)\n",
        "\n",
        "    counter = 0\n",
        "    while counter != list_len:\n",
        "        access = (organic[counter])\n",
        "        #print(\"\")\n",
        "\n",
        "        try:\n",
        "            my_url = (access['url'])\n",
        "            url_list.append(my_url)\n",
        "        except Exception:\n",
        "            url_list.append(\"MISSING\")\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            my_title = (access['title'])\n",
        "            title_list.append(my_title)\n",
        "        except Exception:\n",
        "            title_list.append(\"MISSING\")\n",
        "            pass\n",
        "\n",
        "        query = (resp['query'])\n",
        "        q_access = (query['q'])\n",
        "        query_list.append(q_access)\n",
        "        counter = counter + 1\n",
        "\n",
        "# add lists to dataframe columns\n",
        "df_final['query'] = query_list\n",
        "df_final['url'] = url_list\n",
        "df_final['title'] = title_list\n",
        "\n",
        "df_final[\"query clean\"] = df_final[\"query\"]\n",
        "df_final[\"query clean\"] = (df_final[\"query clean\"].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip())  # strip special chars\n",
        "df_final[\"query clean\"] = df_final[\"query clean\"].str.lower()\n",
        "df_final[\"query clean\"] = df_final[\"query clean\"].replace('\\s+', ' ', regex=True)  # replace whitespace\n",
        "\n",
        "# clean up the page title to test how many times the exact keyword is found\n",
        "df_final[\"title\"] = (df_final[\"title\"].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip())  # strip out all special chars\n",
        "df_final[\"title\"] = df_final[\"title\"].str.lower()\n",
        "df_final[\"title\"] = df_final[\"title\"].replace('\\s+', ' ', regex=True)  # replace whitespace\n",
        "df_final[\"Keyword in Title?\"] = df_final.apply(lambda row: row[\"query clean\"] in row[\"title\"], axis=1)  # kw in title?\n",
        "\n",
        "df_final = df_final[~df_final[\"url\"].isin([\"MISSING\"])]\n",
        "df_final[\"url_count\"] = 1\n",
        "\n",
        "df_grouped = (df_final.groupby(\"query\").agg({\"Keyword in Title?\": \"sum\", \"url_count\": \"sum\"}).reset_index())\n",
        "df_grouped['% of Sites with KW in Title'] = df_grouped['Keyword in Title?'] / df_grouped['url_count'] * 100\n",
        "\n",
        "cols = \"query\", \"url\", \"title\", \"% of Sites with KW in Title\"\n",
        "df_grouped = df_grouped.reindex(columns=cols)\n",
        "\n",
        "# make the stats dataframe\n",
        "df_final_stats = pd.merge(df_final, df_grouped, on=\"query\")\n",
        "del df_final_stats['url_y']\n",
        "del df_final_stats['title_y']\n",
        "\n",
        "# rename the columns\n",
        "df_final_stats.rename(columns={\"url_x\": \"url\", \"title_x\": \"title\"}, inplace=True)\n",
        "\n",
        "# pivot values (tall to wide - so each url has it's own column)\n",
        "df_final_stats['idx'] = df_final_stats.groupby('query').cumcount() + 1\n",
        "df_final_stats = df_final_stats.pivot_table(index=['query', '% of Sites with KW in Title'], columns='idx',\n",
        "                                            values=['url'], aggfunc='first')\n",
        "df_final_stats = df_final_stats.sort_index(axis=1, level=1)\n",
        "df_final_stats.columns = [f'{x}_{y}' for x, y in df_final_stats.columns]\n",
        "df_final_stats = df_final_stats.reset_index()\n",
        "\n",
        "# make the final dataframe\n",
        "df_final_output = pd.merge(df_combined, df_final_stats, left_on=\"Keyword\", right_on=\"query\", how=\"left\")\n",
        "del df_final_output['query']\n",
        "df_final_output['% of Sites with KW in Title'] = df_final_output['% of Sites with KW in Title'].astype(float)\n",
        "df_final_output.round(2)\n",
        "\n",
        "# export the final dataframe\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(red(\"Exported finished_output.csv!\"))\n",
        "print(\"--------------------------------------------------------\")\n",
        "df_final_output.to_csv(\"finished_output.csv\", index=False)\n",
        "files.download('finished_output.csv')\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(red(\"Exported finished_output.csv!\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}